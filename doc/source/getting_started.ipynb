{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started\n",
    "\n",
    "This is only quick overview for getting started. Corpus loading, text preprocessing, etc. are explained in depth in the respective chapters.\n",
    "\n",
    "## Loading a built-in text corpus\n",
    "\n",
    "Once you have installed tmtoolkit, you can start by loading a built-in dataset. Let's import the [Corpus](api.rst#tmtoolkit-corpus) class first and have a look which datasets are available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['english-NewsArticles', 'german-bt18_speeches_sample']"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 1
    }
   ],
   "source": [
    "from tmtoolkit.corpus import Corpus\n",
    "\n",
    "Corpus.builtin_corpora()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load one of these corpora, the [News Articles dataset from Harvard Dataverse](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/GMFCTR):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Corpus [3824 documents]>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 2
    }
   ],
   "source": [
    "corpus = Corpus.from_builtin_corpus('english-NewsArticles')\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can have a look which documents were loaded (showing only the first ten document labels):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['NewsArticles-1',\n 'NewsArticles-2',\n 'NewsArticles-3',\n 'NewsArticles-4',\n 'NewsArticles-5',\n 'NewsArticles-6',\n 'NewsArticles-7',\n 'NewsArticles-8',\n 'NewsArticles-9',\n 'NewsArticles-10']"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 3
    }
   ],
   "source": [
    "corpus.doc_labels[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first 100 characters from the the document `NewsArticles-1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'Betsy DeVos Confirmed as Education Secretary, With Pence Casting Historic Tie-Breaking Vote\\n\\nMichiga'"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 4
    }
   ],
   "source": [
    "corpus['NewsArticles-1'][:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Corpus](api.rst#tmtoolkit-corpus) class is for loading and managing *plain text* corpora, i.e. a set of documents with a label and their content as text strings. It resembles a [Python dictionary](https://docs.python.org/3/tutorial/datastructures.html#dictionaries). See [working with text corpora](text_corpora.ipynb) for more information.\n",
    "\n",
    "\n",
    "## Tokenizing a corpus\n",
    "\n",
    "For quantitative text analysis, you usually work with words in documents as units of interest. This means the plain text strings in the corpus' documents need to be split up into individual *tokens* (words, punctuation, etc.). For a quick starter, we can do so by using [tokenize](api.rst#tmtoolkit.preprocess.tokenize):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from tmtoolkit.preprocess import tokenize\n",
    "\n",
    "doc_labels = corpus.doc_labels   # save the document labels as list for later use\n",
    "\n",
    "docs = tokenize(corpus.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `tokenize()` takes a sequence of text strings, tokenizes them and returns a list of tokenized documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "list"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 6
    }
   ],
   "source": [
    "type(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each document in `docs` in turn is a list of token strings (words, punctuation). Let's peek into the first document (index 0) and return the first ten tokens from it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['Betsy',\n 'DeVos',\n 'Confirmed',\n 'as',\n 'Education',\n 'Secretary',\n ',',\n 'With',\n 'Pence',\n 'Casting']"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 7
    }
   ],
   "source": [
    "docs[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`docs` and `doc_labels` are aligned, i.e. the first element in `doc_labels` is the label of the first tokenized document in `docs`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'NewsArticles-1'"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 8
    }
   ],
   "source": [
    "doc_labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization is part of text preprocessing, which also includes several transformations that you can apply to the tokens (e.g. transform all to lower case). The [chapter on text preprocessing](preprocessing.ipynb) explains this in much more detail."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}