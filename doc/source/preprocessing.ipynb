{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocessing\n",
    "\n",
    "During text preprocessing, a corpus of documents is tokenized (i.e. the document strings are split into individual words, punctuation, numbers, etc.) and then these tokens can be transformed, filtered or annotated. The goal is to prepare the raw texts in a way that makes it easier to perform eventual analysis methods in a later stage, e.g. by reducing noise in the dataset. tmtoolkit provides a rich set of tools for this purpose in the [tmtoolkit.preprocess](api.rst#tmtoolkit-preprocess) module.   \n",
    "\n",
    "## Two approaches: functional API and TMPreproc class\n",
    "\n",
    "There are two ways to apply text preprocessing methods to your documents: First, there is the [functional API](api.rst#module-tmtoolkit.preprocess) which consists of a set of Python functions that accept a list of (tokenized) documents. An example might be:\n",
    "\n",
    "```python\n",
    "corpus = [\n",
    "    \"Hello world!\",    # document 1\n",
    "    \"Another example\"  # document 2\n",
    "]\n",
    "\n",
    "docs = tokenize(corpus)\n",
    "to_lowercase(docs)\n",
    "# Out: [['hello', 'world', '!'],\n",
    "#       ['another', 'example']]\n",
    "```\n",
    "\n",
    "\n",
    "The advantage of this approach is that it's very straight-forward and flexible. However, you must manage any meta data associated with the documents on your own (e.g. document labels or token metadata). Furthermore, the processing is not done in parallel.\n",
    "\n",
    "Second, there is the [TMPreproc class](api.rst#tmpreproc-class-for-parallel-text-preprocessing) which addresses these limitations. You can create an instance of this class from your (labelled) documents and then apply preprocessing methods to it. This instance is a \"state-machine\", i.e. its contents (the documents) can change when you call a method. An example:\n",
    "\n",
    "```python\n",
    "corpus = {\n",
    "    \"doc1\": \"Hello world!\",\n",
    "    \"doc2\": \"Another example\"\n",
    "}\n",
    "\n",
    "preproc = TMPreproc(corpus)     # documents are directly tokenized\n",
    "preproc.tokens_to_lowercase()   # this changes the documents\n",
    "preproc.tokens                  # one of many ways to access the tokens\n",
    "\n",
    "# Out:\n",
    "# {\n",
    "#   'doc1': ['hello', 'world', '!'],\n",
    "#   'doc2': ['another', 'example']\n",
    "# }\n",
    "```\n",
    "\n",
    "The most important advantage is that `TMPreproc` employs parallel processing, i.e. it uses all available processors on your machine to do the computations necessary during preprocessing. For large text corpora, this can lead to a strong speed up. \n",
    "\n",
    "Both approaches offer mostly the same features in terms of available preprocessing methods. `TMPreproc` has some more methods to export the data to dataframes or datatables. In general, the functional API is mostly used for quick prototyping and when using a small amount of data. For projects with large amounts of data, it's recommended to use `TMPreproc`.\n",
    "\n",
    "This chapter starts with a few examples using the functional API and then turns to `TMPreproc`.\n",
    "\n",
    "## Functional API\n",
    "\n",
    "The functions in the preprocessing module make up the [functional API](api.rst#module-tmtoolkit.preprocess) for text preprocessing. We will explore some of the available functions. Most of them require at least passing a list of tokenized documents. In order to tokenize raw text documents (for example from a [Corpus](text_corpora.ipynb) object), we can use [tokenize()](tmtoolkit.preprocess.tokenize): \n",
    "\n",
    "Let's load sample of three documents from the built-in *NewsArticles* dataset. We'll save the document labels in `doc_labels` since the functional API only works with lists of documents (not with dicts): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "dict_keys(['NewsArticles-774', 'NewsArticles-1647', 'NewsArticles-1396'])"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 7
    }
   ],
   "source": [
    "from tmtoolkit.corpus import Corpus\n",
    "from tmtoolkit.preprocess import tokenize\n",
    "\n",
    "corpus = Corpus.from_builtin_corpus('english-NewsArticles').sample(3)\n",
    "doc_labels = corpus.keys()\n",
    "doc_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can now tokenize these documents. We use `corpus.values()` to pass a list of documents. We get a list of tokenized documents back (i.e. a list of lists). We peak into the documents by only showing the first 10 words at maximum."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "[['Russian',\n  'adventurer',\n  'Konyukhov',\n  \"'s\",\n  'balloon',\n  'lands',\n  'after',\n  '55-hour',\n  'nonstop',\n  'flight'],\n ['Fallen',\n  'Navy',\n  'SEAL',\n  \"'s\",\n  'widow',\n  'receives',\n  'standing',\n  'ovation',\n  'during',\n  'Trump'],\n ['Troops',\n  'advance',\n  'towards',\n  'Ghazlani',\n  'base',\n  'near',\n  'Mosul',\n  'airport',\n  'Iraqi',\n  'troops']]"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 10
    }
   ],
   "source": [
    "docs = tokenize(corpus.values())\n",
    "[doc[:10] for doc in docs]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "language\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}